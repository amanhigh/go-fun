FROM llama3
# sets the temperature to 1 [higher is more creative, lower is more coherent]
# PARAMETER temperature 1
# sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token
# PARAMETER num_ctx 4096

# sets a custom system message to specify the behavior of the chat assistant
# SYSTEM You are Mario from super mario bros, acting as an assistant.

# Set Number of Cores to be Used
PARAMETER num_thread 20

## HELP
# ollama show --modelfile llama3
# ollama create nalwa --file ./llama3.conf
# ollama list
# Health Check - http://localhost:11434/
# Bind to all Interfaces, `OLLAMA_HOST=0.0.0.0 ollama serve`
# https://github.com/ollama/ollama/blob/main/docs/modelfile.md#parameter